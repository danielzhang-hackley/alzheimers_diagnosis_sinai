{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'../data/X_expr.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = pd.read_csv(r'../data/y_cog.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = label_encoder.fit_transform(y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train, y_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, y_train)\n",
    ").shuffle(10000).batch(100)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_encoder(input_shape, layer_sizes, activation='relu'):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    for i, n_nodes in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(input_layer)\n",
    "        else:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_cnn_encoder(input_shape, filters, kernel_sizes, dnn_layer_sizes, strides=None, paddings=\"valid\"):\n",
    "    if strides is None:\n",
    "        strides = [(1, 1) for _ in range(len(filters))]\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    for i in range(len(filters)):\n",
    "        if i == 0:\n",
    "            x = layers.Conv2D(filters[i], kernel_sizes[i], strides[i])(input_layer)\n",
    "        else:\n",
    "            x = layers.Conv2D(filters[i], kernel_sizes[i], strides[i])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    pre_flatten_idx = len(filters) - 1\n",
    "    flatten_idx = len(filters)\n",
    "\n",
    "    for i in range(len(dnn_layer_sizes)):\n",
    "        x = layers.Dense(dnn_layer_sizes[i])(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    pre_flatten_dim = model.layers[pre_flatten_idx].output_shape\n",
    "    n_flatten_nodes = model.layers[flatten_idx].output_shape\n",
    "\n",
    "    return model, pre_flatten_dim, n_flatten_nodes\n",
    "\n",
    "\n",
    "def get_rf_encoder(X_train, y_train, n_estimators=1000, random_state=None):\n",
    "    \"\"\"\n",
    "    trees = model.estimators_\n",
    "    # Get all 50 tree predictions for the first sample in X_train\n",
    "    preds_for_0 = [tree.predict(X_train[0].reshape(1, -1))[0] for tree in trees]\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_decoder(latent_shape, layer_sizes, activation='relu'):\n",
    "    \"\"\"\n",
    "    Not strictly for decoding; can also be used for classification\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=latent_shape)\n",
    "    for i, n_nodes in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(input_layer)\n",
    "        elif i == len(layer_sizes) - 1:\n",
    "            x = layers.Dense(n_nodes, activation=tf.keras.activations.softmax)(x)\n",
    "        else:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_cnn_decoder(latent_shape):\n",
    "    pass\n",
    "\n",
    "def get_svm_decoder(latent_shape):\n",
    "    pass\n",
    "\n",
    "def get_logistic_decoder(latent_shape):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "loss_object_sparse = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "loss_object_autoencoder = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(data, training=True)\n",
    "        loss = loss_object_sparse(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy_sparse(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, data, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = loss_object_sparse(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy_sparse(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_autoencoder(encoder, decoder, data, labels):\n",
    "    with tf.GradientTape() as enc, tf.GradientTape() as dec:\n",
    "        latent = encoder(data, training=True)\n",
    "        generated_data = decoder(latent, training=True)\n",
    "        loss = loss_object_autoencoder(data, generated_data)\n",
    "    encoder_gradient = enc.gradient(loss, encoder.trainable_variables)\n",
    "    decoder_gradient = dec.gradient(loss, decoder.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(encoder_gradient, encoder.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(decoder_gradient, decoder.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_AE = 50\n",
    "\n",
    "layer_sizes = [18980, 10000, 2500, 2500, 400]\n",
    "layer_sizes_reversed = list(reversed(layer_sizes))\n",
    "encoder = get_nn_encoder((layer_sizes[0]), layer_sizes=layer_sizes[1:])\n",
    "decoder = get_nn_decoder((layer_sizes_reversed[0]), layer_sizes=layer_sizes_reversed[1:])\n",
    "\n",
    "for epoch in range(EPOCHS_AE):\n",
    "    start_time = time.time()\n",
    "    for data, labels in train_ds:\n",
    "        loss = train_step_autoencoder(encoder, decoder, data, labels)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss}, Time for epoch: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# model = get_nn_decoder((400), layer_sizes=[300, 150, 150, 50, 3])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy_sparse.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy_sparse.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = encoder.predict(data)\n",
    "        train_step(model, encoded_data, labels)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = encoder.predict(test_data)\n",
    "        test_step(model, encoded_test_data, test_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy_sparse.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy_sparse.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in train_ds:\n",
    "    print(type(data))\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 1000\n",
    "rf_encoder = get_rf_encoder(X_train, y_train, n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0200287103652954, Accuracy: 43.1297721862793, Test Loss: 1.1670171022415161, Test Accuracy: 36.66666793823242\n",
      "Epoch 2, Loss: 0.9879631400108337, Accuracy: 43.511451721191406, Test Loss: 1.168146014213562, Test Accuracy: 40.0\n",
      "Epoch 3, Loss: 0.9739248156547546, Accuracy: 43.511451721191406, Test Loss: 1.1680554151535034, Test Accuracy: 40.0\n",
      "Epoch 4, Loss: 0.9544284343719482, Accuracy: 43.89312744140625, Test Loss: 1.1672734022140503, Test Accuracy: 40.0\n",
      "Epoch 5, Loss: 0.9476329684257507, Accuracy: 44.274810791015625, Test Loss: 1.1661981344223022, Test Accuracy: 40.0\n",
      "Epoch 6, Loss: 0.9412970542907715, Accuracy: 44.274810791015625, Test Loss: 1.1646977663040161, Test Accuracy: 40.0\n",
      "Epoch 7, Loss: 0.9238945841789246, Accuracy: 45.03816604614258, Test Loss: 1.1619668006896973, Test Accuracy: 40.0\n",
      "Epoch 8, Loss: 0.9109888076782227, Accuracy: 45.41984939575195, Test Loss: 1.1606254577636719, Test Accuracy: 40.0\n",
      "Epoch 9, Loss: 0.9015046954154968, Accuracy: 45.8015251159668, Test Loss: 1.1602338552474976, Test Accuracy: 40.0\n",
      "Epoch 10, Loss: 0.8901289105415344, Accuracy: 46.183204650878906, Test Loss: 1.159968614578247, Test Accuracy: 40.0\n",
      "Epoch 11, Loss: 0.8926048874855042, Accuracy: 46.183204650878906, Test Loss: 1.1605316400527954, Test Accuracy: 40.0\n",
      "Epoch 12, Loss: 0.8686785697937012, Accuracy: 46.564884185791016, Test Loss: 1.1594433784484863, Test Accuracy: 40.0\n",
      "Epoch 13, Loss: 0.8580775260925293, Accuracy: 47.3282470703125, Test Loss: 1.1582224369049072, Test Accuracy: 40.0\n",
      "Epoch 14, Loss: 0.8574316501617432, Accuracy: 47.709922790527344, Test Loss: 1.1560884714126587, Test Accuracy: 40.0\n",
      "Epoch 15, Loss: 0.8558631539344788, Accuracy: 47.709922790527344, Test Loss: 1.1530802249908447, Test Accuracy: 40.0\n",
      "Epoch 16, Loss: 0.8479328751564026, Accuracy: 48.85496139526367, Test Loss: 1.1500109434127808, Test Accuracy: 40.0\n",
      "Epoch 17, Loss: 0.8507412075996399, Accuracy: 49.61832046508789, Test Loss: 1.1473147869110107, Test Accuracy: 40.0\n",
      "Epoch 18, Loss: 0.8336160778999329, Accuracy: 50.38167953491211, Test Loss: 1.1448190212249756, Test Accuracy: 40.0\n",
      "Epoch 19, Loss: 0.8316817283630371, Accuracy: 50.76335525512695, Test Loss: 1.1440602540969849, Test Accuracy: 40.0\n",
      "Epoch 20, Loss: 0.8293824195861816, Accuracy: 51.52671813964844, Test Loss: 1.1432433128356934, Test Accuracy: 40.0\n",
      "Epoch 21, Loss: 0.8260741233825684, Accuracy: 51.52671813964844, Test Loss: 1.1414917707443237, Test Accuracy: 40.0\n",
      "Epoch 22, Loss: 0.8085289597511292, Accuracy: 51.90839767456055, Test Loss: 1.1401361227035522, Test Accuracy: 43.33333206176758\n",
      "Epoch 23, Loss: 0.8087838292121887, Accuracy: 52.290077209472656, Test Loss: 1.1405762434005737, Test Accuracy: 43.33333206176758\n",
      "Epoch 24, Loss: 0.8014623522758484, Accuracy: 52.6717529296875, Test Loss: 1.1411129236221313, Test Accuracy: 43.33333206176758\n",
      "Epoch 25, Loss: 0.7958753108978271, Accuracy: 53.816795349121094, Test Loss: 1.141643762588501, Test Accuracy: 43.33333206176758\n"
     ]
    }
   ],
   "source": [
    "# 620, -620\n",
    "EPOCHS = 25\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "decoder = get_nn_decoder((1000),  [500, 200, 100, 10, 3])\n",
    "\n",
    "\n",
    "def rf_encode(encoder, data):\n",
    "    \"\"\"\n",
    "    trees = model.estimators_\n",
    "    # Get all 50 tree predictions for the first sample in X_train\n",
    "    preds_for_0 = [tree.predict(X_train[0].reshape(1, -1))[0] for tree in trees]\n",
    "    \"\"\"\n",
    "    trees = encoder.estimators_\n",
    "    n_estimators = len(trees)\n",
    "\n",
    "    output = np.zeros(shape=(data.shape[0], n_estimators))\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        output[i] = [tree.predict(sample.numpy().reshape(1, -1))[0] for tree in trees]\n",
    "\n",
    "    return tf.convert_to_tensor(output)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy_sparse.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy_sparse.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = rf_encode(rf_encoder, data)\n",
    "        train_step(decoder, encoded_data, labels)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = rf_encode(rf_encoder, test_data)\n",
    "        test_step(decoder, encoded_test_data, test_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy_sparse.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy_sparse.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb92a5de44aaf59754d1a7f59abdbfeec2642db842c8064b433264bae7e1adfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
