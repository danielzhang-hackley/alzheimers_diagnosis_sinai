{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'../data/X_expr.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = pd.read_csv(r'../data/y_cog.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = label_encoder.fit_transform(y.ravel())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AD', 'MildCognitiveImpairment', 'NoCognitiveImpairment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train, y_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, y_train)\n",
    ").shuffle(10000).batch(100)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and decoder definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_encoder(input_shape, filters, kernel_sizes, bottleneck_size,\n",
    "                    dnn_layer_sizes=None, strides=None, paddings=\"same\", activation='relu'):\n",
    "    if strides is None:\n",
    "        strides = [(1, 1) for _ in range(len(filters))]\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(filters[0], kernel_size=kernel_sizes[0], strides=strides[0], padding=paddings)(input_layer)\n",
    "\n",
    "    for i in range(1, len(filters)):\n",
    "        x = layers.Conv2D(filters[i], kernel_size=kernel_sizes[i], strides=strides[i], padding=paddings)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "\n",
    "    if dnn_layer_sizes is not None:\n",
    "        for n_nodes in dnn_layer_sizes:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "\n",
    "    x = layers.Dense(bottleneck_size, activation=activation)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    flatten_idx = 1\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.name == 'flatten':\n",
    "            flatten_idx = i\n",
    "    pre_flatten_idx = flatten_idx - 1\n",
    "\n",
    "    pre_flatten_dim = model.layers[pre_flatten_idx].output_shape\n",
    "    n_flatten_nodes = model.layers[flatten_idx].output_shape\n",
    "\n",
    "    return model, pre_flatten_dim, n_flatten_nodes\n",
    "\n",
    "\n",
    "def get_cnn_decoder(bottleneck_size, pre_flatten_dim, n_flatten_nodes, filters, kernel_sizes, \n",
    "                    dnn_layer_sizes=None, strides=None, paddings=\"same\", activation='relu'):\n",
    "    if pre_flatten_dim[-1] != filters[0]:\n",
    "        raise ValueError(\"filter sizes do not match encoder dimensions\")\n",
    "   \n",
    "    if strides is None:\n",
    "        strides = [(1, 1) for _ in range(len(filters))]\n",
    "\n",
    "    input_layer = layers.Input(shape=(bottleneck_size))\n",
    "\n",
    "    if dnn_layer_sizes is not None:\n",
    "        x = layers.Dense(dnn_layer_sizes[0], activation=activation)(input_layer)\n",
    "        for n_nodes in dnn_layer_sizes[1:]:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "        x = layers.Dense(n_flatten_nodes)(x)\n",
    "    else:\n",
    "        x = layers.Dense(n_flatten_nodes)(input_layer)\n",
    "    \n",
    "    x = layers.Reshape(pre_flatten_dim)(x)\n",
    "\n",
    "    new_filters = filters[1:] + [1]\n",
    "    for i in range(len(new_filters) - 1):\n",
    "        x = layers.Conv2DTranspose(new_filters[i], kernel_size=kernel_sizes[i], strides=strides[i], padding=paddings)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2DTranspose(new_filters[-1], kernel_size=kernel_sizes[-1], strides=strides[-1], padding=paddings, \n",
    "                               activation=tf.keras.activations.sigmoid)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and decoder initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_size = 10\n",
    "\n",
    "filters = [32, 64, 64, 64]\n",
    "kernel_sizes = [3, 3, 3, 3]\n",
    "strides = [1, 2, 2, 1]\n",
    "\n",
    "filters_r = list(reversed(filters))\n",
    "kernel_sizes_r = list(reversed(kernel_sizes))\n",
    "strides_r = list(reversed(strides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc, pre_flatten_dim, n_flatten_nodes = get_cnn_encoder(\n",
    "    input_shape=(28, 28, 1), \n",
    "    bottleneck_size=bottleneck_size,\n",
    "    filters=filters, \n",
    "    kernel_sizes=kernel_sizes, \n",
    "    strides=strides\n",
    ")\n",
    "\n",
    "dec = get_cnn_decoder(\n",
    "    bottleneck_size=bottleneck_size, \n",
    "    pre_flatten_dim=pre_flatten_dim[1:], \n",
    "    n_flatten_nodes=n_flatten_nodes[1],\n",
    "    filters=filters_r,\n",
    "    kernel_sizes=kernel_sizes_r,\n",
    "    strides=strides_r\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "@tf.function\n",
    "def train_step_cnn_autoencoder(images):\n",
    " \n",
    "    with tf.GradientTape() as tape_encoder, tf.GradientTape() as tape_decoder:\n",
    "       \n",
    "        latent = enc(images, training=True)\n",
    "        generated_images = dec(latent, training=True)\n",
    "        loss = loss_object(images, generated_images)\n",
    "         \n",
    "    gradients_of_enc = tape_encoder.gradient(loss, enc.trainable_variables)\n",
    "    gradients_of_dec = tape_decoder.gradient(loss, dec.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step_cnn_autoencoder(images):\n",
    "    predictions = dec(enc(images, training=False), training=False)\n",
    "    t_loss = loss_object(images, predictions)\n",
    "\n",
    "    test_loss(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        train_step_cnn_autoencoder(data)\n",
    "        print(f'epoch: {epoch + 1} | train loss: {train_loss.result()}', end='\\r')\n",
    "    print()\n",
    "\n",
    "    for data, test_labels in test_ds:\n",
    "        test_step_cnn_autoencoder(data)\n",
    "        print(f'epoch: {epoch + 1} | test loss: {test_loss.result()}', end='\\r')\n",
    "    print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_decoder(latent_shape, layer_sizes, dropout_rates=None, activation='relu'):\n",
    "    # dense, dropout, dense, dropout, dense\n",
    "\n",
    "    input_layer = layers.Input(shape=latent_shape)\n",
    "    x = layers.Dense(layer_sizes[0], activation=activation)(input_layer)\n",
    "\n",
    "    for i, n_nodes in enumerate(layer_sizes[1: ], 1):\n",
    "        if dropout_rates is not None:\n",
    "            x = layers.Dropout(dropout_rates[i - 1])(x)\n",
    "\n",
    "        if i == len(layer_sizes) - 1:\n",
    "            x = layers.Dense(n_nodes, activation=tf.keras.activations.softmax)(x)\n",
    "        else:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)  \n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_nn_decoder(bottleneck_size, [500, 500, 100, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object_sparse = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step_nn(data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        latent = enc(data, training=True)\n",
    "        predictions = classifier(latent, training=True)\n",
    "        loss = loss_object_sparse(labels, predictions)\n",
    "    gradients = tape.gradient(loss, classifier.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, classifier.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy_sparse(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step_nn(data, labels):\n",
    "    latent = enc(data, training=False)\n",
    "    predictions = classifier(data, training=False)\n",
    "    t_loss = loss_object_sparse(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy_sparse(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy_sparse.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy_sparse.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = enc(data)\n",
    "        train_step_nn(encoded_data, labels)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = enc(test_data)\n",
    "        test_step_nn(encoded_test_data, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9960baf88259386db57c734c8604c8e4ab789688672644b3cf73fda24b112c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
