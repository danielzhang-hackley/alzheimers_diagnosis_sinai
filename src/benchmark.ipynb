{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:25:02.454216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-23 17:25:02.454229: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDNN(tf.keras.Model):\n",
    "    def __init__(self, node_counts, dropout_ratios=None, \n",
    "                 activation='relu', optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)):\n",
    "        super(BaseDNN, self).init__()\n",
    "        self.node_counts = node_counts\n",
    "        self.dropout_ratios = [0 for _ in range(len(self.node_counts))] if dropout_ratios is None else dropout_ratios\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.optimizer=optimizer\n",
    "        self.loss=loss\n",
    "\n",
    "    def call(self, x):\n",
    "        for n_nodes, dropout_ratio in zip(self.node_counts, self.dropout_ratios):\n",
    "            x = layers.Dense(n_nodes, activation=self.activation)(x)\n",
    "            x = layers.Dropout(dropout_ratio)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(data, training=True)\n",
    "        loss = model.loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, data, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = model.loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "def fit_transform_one_hot(arr):\n",
    "    output = np.zeros([arr.shape[0]])\n",
    "    for i in range(len(output)):\n",
    "        output[i] = np.argmax(arr[i])\n",
    "    return output\n",
    "\n",
    "label_encoder.fit_transform_one_hot = fit_transform_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'../data/X_expr.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = pd.read_csv(r'../data/y_cog.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "\n",
    "for i, patient in enumerate(y):\n",
    "    if y[i][0] == \"MildCognitiveImpairment\":\n",
    "        y[i][0] = \"AD\"\n",
    "\n",
    "\"\"\"y_class1_idx = np.where(y == 'NoCognitiveImpairment')[0]\n",
    "y_class2_idx = np.where(y == 'MildCognitiveImpairment')[0]\n",
    "y_class3_idx = np.where(y == 'AD')[0]\n",
    "\n",
    "# smallest class is MildCognitiveImpairment, with 73 samples\n",
    "y_class1_idx_sub = np.random.choice(y_class1_idx, 73, replace=False)\n",
    "y_class2_idx_sub = np.random.choice(y_class2_idx, 73, replace=False)\n",
    "y_class3_idx_sub = np.random.choice(y_class3_idx, 73, replace=False)\n",
    "\n",
    "bal_idx = np.hstack([y_class1_idx_sub, y_class2_idx_sub, y_class3_idx_sub])\n",
    "\n",
    "X = X[bal_idx]\n",
    "y = y[bal_idx]\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "0\n",
      "177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['NoCognitiveImpairment'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['AD'],\n",
       "       ['NoCognitiveImpairment']], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.where(y == 'NoCognitiveImpairment')[0].shape[0])\n",
    "print(np.where(y == 'MildCognitiveImpairment')[0].shape[0])\n",
    "print(np.where(y == 'AD')[0].shape[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = standard_scaler.fit_transform(X_train, y_train)\n",
    "X_test_normalized = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_classes = 2  # y_train.shape[1]\n",
    "\n",
    "# nfe 7, 67, 72\n",
    "# np 121, 79, -82\n",
    "\n",
    "lr_l1 = LogisticRegression(penalty=\"l2\", multi_class=\"ovr\", solver=\"saga\", max_iter=500)\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "dnn = keras.Sequential([\n",
    "    layers.Dense(n_features),\n",
    "    layers.Dense(9000),\n",
    "    layers.Dense(2000),\n",
    "    layers.Dense(100),\n",
    "    layers.Dense(n_classes)\n",
    "])\n",
    "dnn.compile(optimizer='adam', \n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "\n",
    "classical_models = (lr_l1, svm, rf)\n",
    "\n",
    "y_train_sparse = label_encoder.fit_transform(y_train.ravel())\n",
    "y_test_sparse = label_encoder.transform(y_test.ravel())\n",
    "\n",
    "\n",
    "base_dnn = BaseDNN([n_features, 9000, 2000, 100, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.36666666666666664\n",
      "[[4 4 3]\n",
      " [4 3 0]\n",
      " [2 6 4]]\n",
      "********************\n",
      "acc: 0.3\n",
      "[[ 0  0 11]\n",
      " [ 0  0  7]\n",
      " [ 3  0  9]]\n",
      "********************\n",
      "acc: 0.4\n",
      "[[4 0 7]\n",
      " [2 0 5]\n",
      " [4 0 8]]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "for model in classical_models:\n",
    "    model.fit(X_train_normalized, y_train_sparse)\n",
    "    y_pred = model.predict(X_test_normalized).ravel()\n",
    "    \n",
    "    cnf = confusion_matrix(y_test_sparse, y_pred)\n",
    "    acc = accuracy_score(y_test_sparse, y_pred)\n",
    "    # f1 = f1_score(y_test_sparse, y_pred)\n",
    "\n",
    "    print(f\"acc: {acc}\")  # , f1: {f1}\")\n",
    "    print(cnf)\n",
    "    print(\"********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 13s 1s/step - loss: 71282.3359 - categorical_accuracy: 0.4580\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 12s 1s/step - loss: 18865.6484 - categorical_accuracy: 0.4962\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 12s 1s/step - loss: 48746.2109 - categorical_accuracy: 0.4695\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 12s 1s/step - loss: 26779.5625 - categorical_accuracy: 0.5496\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 12s 1s/step - loss: 15286.3975 - categorical_accuracy: 0.4924\n",
      "9/9 [==============================] - 1s 132ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "acc: 0.4122137404580153\n",
      "[[  5 154]\n",
      " [  0 103]]\n",
      "********************\n",
      "acc: 0.4\n",
      "[[ 0 18]\n",
      " [ 0 12]]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "y_train_one_hot = to_categorical(y_train_sparse)\n",
    "y_test_one_hot = to_categorical(y_test_sparse)\n",
    "# print(y_train_one_hot)\n",
    "\n",
    "dnn.fit(X_train, y_train_one_hot, epochs=5)\n",
    "y_pred_train = dnn.predict(X_train)\n",
    "y_pred = dnn.predict(X_test)\n",
    "\n",
    "y_pred_sparse_train = label_encoder.fit_transform_one_hot(y_pred_train)\n",
    "y_pred_sparse = label_encoder.fit_transform_one_hot(y_pred)\n",
    "\n",
    "cnf_train = confusion_matrix(y_train_sparse, y_pred_sparse_train)\n",
    "acc_train = accuracy_score(y_train_sparse, y_pred_sparse_train)\n",
    "\n",
    "print(f\"acc: {acc_train}\")\n",
    "print(cnf_train)\n",
    "print(\"********************\")\n",
    "\n",
    "cnf_test = confusion_matrix(y_test_sparse, y_pred_sparse)\n",
    "acc_test = accuracy_score(y_test_sparse, y_pred_sparse)\n",
    "\n",
    "print(f\"acc: {acc_test}\")\n",
    "print(cnf_test)\n",
    "print(\"********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_test_sparse == 2)[0].shape[0]\n",
    "\n",
    "# 133, 61, -77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def encoder(input_encoder):\n",
    "\tinputs = keras.Input(shape=input_encoder, name='input_layer')\n",
    "\n",
    "\t# Block-1\n",
    "\tx = layers.Dense(100, name='conv_1')(inputs)\n",
    "\tx = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "\n",
    "\t# Block-2\n",
    "\tx = layers.Dense(75, name='conv_2')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "\n",
    "\t# Block-3\n",
    "\tx = layers.Dense(75, name='conv_3')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "\n",
    "\n",
    "\t# Block-4\n",
    "\tx = layers.Dense(50, name='conv_4')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_4')(x)\n",
    "\n",
    "\t# Final Block\n",
    "\tflatten = x\n",
    "\tmean = layers.Dense(2, name='mean')(flatten)\n",
    "\tlog_var = layers.Dense(2, name='log_var')(flatten)\n",
    "\tmodel = tf.keras.Model(inputs, (mean, log_var), name=\"Encoder\")\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def sampling(input_1,input_2):\n",
    "\tmean = keras.Input(shape=input_1, name='input_layer1')\n",
    "\tlog_var = keras.Input(shape=input_2, name='input_layer2')\n",
    "\tout = layers.Lambda(sampling_reparameterization_model, name='encoder_output')([mean, log_var])\n",
    "\tenc_2 = tf.keras.Model([mean,log_var], out,  name=\"Encoder_2\")\n",
    "\n",
    "\treturn enc_2\n",
    "\n",
    "\n",
    "def sampling_reparameterization_model(distribution_params):\n",
    "    mean, log_var = distribution_params\n",
    "    epsilon = K.random_normal(shape=K.shape(mean), mean=0., stddev=1.)\n",
    "    z = mean + K.exp(log_var / 2) * epsilon\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "# replace the decoder with the supervised learning method\n",
    "def decoder(input_decoder):\n",
    "\t\t\n",
    "\tinputs = keras.Input(shape=input_decoder, name='input_layer')\n",
    "\tx = layers.Dense(50, name='dense_1')(inputs)\n",
    "\t\n",
    "\t# Block-1\n",
    "\tx = layers.Dense(75,name='conv_transpose_1')(x)\n",
    "\tx = layers.BatchNormalization(name='bn_1')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "\t\n",
    "\t# Block-2\n",
    "\tx = layers.Dense(75, name='conv_transpose_2')(x)\n",
    "\tx = layers.BatchNormalization(name='bn_2')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_2')(x)\t\t\n",
    "\n",
    "\t# INSERT NORMAL SUPERVISED LEARNING HERE?\n",
    "\t# Block-4\n",
    "\toutputs = layers.Dense(100, name='conv_transpose_4')(x)\n",
    "\tmodel = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
    "\treturn model\t\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 0.0005)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "\tr_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\treturn 1000 * r_loss\n",
    "\t\n",
    "\n",
    "def kl_loss(mean, log_var):\n",
    "\tkl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis = 1)\n",
    "\treturn kl_loss\n",
    "\t\n",
    "\n",
    "def vae_loss(y_true, y_pred, mean, log_var):\n",
    "\tr_loss = mse_loss(y_true, y_pred)\n",
    "\tkl_loss = kl_loss(mean, log_var)\n",
    "\treturn  r_loss + kl_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "\t\n",
    "\twith tf.GradientTape() as enc, tf.GradientTape() as dec:\n",
    "\t\t\n",
    "\t\tmean, log_var = enc(images, training=True)\n",
    "\t\tlatent = sampling([mean, log_var])\n",
    "\t\tgenerated_images = dec(latent, training=True)\n",
    "\t\tloss = vae_loss(images, generated_images, mean, log_var)\n",
    "\t\n",
    "\t\t\t\n",
    "\tgradients_of_enc = encoder.gradient(loss, enc.trainable_variables)\n",
    "\tgradients_of_dec = decoder.gradient(loss, dec.trainable_variables)\n",
    "\t\t\n",
    "\t\t\n",
    "\toptimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
    "\toptimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
    "\treturn loss\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tstart = time.time()\n",
    "\t\tfor image_batch in dataset:\n",
    "\t\t\ttrain_step(image_batch)\n",
    "\t\n",
    "\tprint ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb92a5de44aaf59754d1a7f59abdbfeec2642db842c8064b433264bae7e1adfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
