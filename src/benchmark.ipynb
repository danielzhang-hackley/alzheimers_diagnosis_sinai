{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_sinai/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import backend as K\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_encoder):\n",
    "\tinputs = keras.Input(shape=input_encoder, name='input_layer')\n",
    "\n",
    "\t# Block-1\n",
    "\tx = layers.Dense(100, name='conv_1')(inputs)\n",
    "\tx = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "\n",
    "\t# Block-2\n",
    "\tx = layers.Dense(75, name='conv_2')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "\n",
    "\t# Block-3\n",
    "\tx = layers.Dense(75, name='conv_3')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "\n",
    "\n",
    "\t# Block-4\n",
    "\tx = layers.Dense(50, name='conv_4')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_4')(x)\n",
    "\n",
    "\t# Final Block\n",
    "\tflatten = x\n",
    "\tmean = layers.Dense(2, name='mean')(flatten)\n",
    "\tlog_var = layers.Dense(2, name='log_var')(flatten)\n",
    "\tmodel = tf.keras.Model(inputs, (mean, log_var), name=\"Encoder\")\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def sampling(input_1,input_2):\n",
    "\tmean = keras.Input(shape=input_1, name='input_layer1')\n",
    "\tlog_var = keras.Input(shape=input_2, name='input_layer2')\n",
    "\tout = layers.Lambda(sampling_reparameterization_model, name='encoder_output')([mean, log_var])\n",
    "\tenc_2 = tf.keras.Model([mean,log_var], out,  name=\"Encoder_2\")\n",
    "\n",
    "\treturn enc_2\n",
    "\n",
    "\n",
    "def sampling_reparameterization_model(distribution_params):\n",
    "    mean, log_var = distribution_params\n",
    "    epsilon = K.random_normal(shape=K.shape(mean), mean=0., stddev=1.)\n",
    "    z = mean + K.exp(log_var / 2) * epsilon\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "# replace the decoder with the supervised learning method\n",
    "def decoder(input_decoder):\n",
    "\t\t\n",
    "\tinputs = keras.Input(shape=input_decoder, name='input_layer')\n",
    "\tx = layers.Dense(50, name='dense_1')(inputs)\n",
    "\t\n",
    "\t# Block-1\n",
    "\tx = layers.Dense(75,name='conv_transpose_1')(x)\n",
    "\tx = layers.BatchNormalization(name='bn_1')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "\t\n",
    "\t# Block-2\n",
    "\tx = layers.Dense(75, name='conv_transpose_2')(x)\n",
    "\tx = layers.BatchNormalization(name='bn_2')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_2')(x)\t\t\n",
    "\n",
    "\t# INSERT NORMAL SUPERVISED LEARNING HERE?\n",
    "\t# Block-4\n",
    "\toutputs = layers.Dense(100, name='conv_transpose_4')(x)\n",
    "\tmodel = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
    "\treturn model\t\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 0.0005)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "\tr_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\treturn 1000 * r_loss\n",
    "\t\n",
    "\n",
    "def kl_loss(mean, log_var):\n",
    "\tkl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis = 1)\n",
    "\treturn kl_loss\n",
    "\t\n",
    "\n",
    "def vae_loss(y_true, y_pred, mean, log_var):\n",
    "\tr_loss = mse_loss(y_true, y_pred)\n",
    "\tkl_loss = kl_loss(mean, log_var)\n",
    "\treturn  r_loss + kl_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "\t\n",
    "\twith tf.GradientTape() as enc, tf.GradientTape() as dec:\n",
    "\t\t\n",
    "\t\tmean, log_var = enc(images, training=True)\n",
    "\t\tlatent = sampling([mean, log_var])\n",
    "\t\tgenerated_images = dec(latent, training=True)\n",
    "\t\tloss = vae_loss(images, generated_images, mean, log_var)\n",
    "\t\n",
    "\t\t\t\n",
    "\tgradients_of_enc = encoder.gradient(loss, enc.trainable_variables)\n",
    "\tgradients_of_dec = decoder.gradient(loss, dec.trainable_variables)\n",
    "\t\t\n",
    "\t\t\n",
    "\toptimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
    "\toptimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
    "\treturn loss\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tstart = time.time()\n",
    "\t\tfor image_batch in dataset:\n",
    "\t\t\ttrain_step(image_batch)\n",
    "\t\n",
    "\tprint ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 15:44:49.642431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 15:44:49.642699: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642747: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642824: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642900: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642973: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-18 15:44:49.642980: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-18 15:44:49.643406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "input_size = 256\n",
    "n_classes = 3\n",
    "\n",
    "lr_l1 = LogisticRegression(penalty=\"l1\")\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# 1400, 300\n",
    "# 1130, 300\n",
    "# 1260, 190\n",
    "# 1000, 280\n",
    "# 185, 50 (n)\n",
    "# 1111, 635\n",
    "# 1008, 454\n",
    "# 1005, 448 (nw)\n",
    "# 180, 566 (n f)\n",
    "# 190, 400 (n t)\n",
    "# 180 630 (n s)\n",
    "dnn = keras.Sequential([\n",
    "    layers.Dense(input_size),\n",
    "    layers.Dense(100),\n",
    "    layers.Dense(75),\n",
    "    layers.Dense(75),\n",
    "    layers.Dense(50),\n",
    "    layers.Dense(n_classes)\n",
    "])\n",
    "dnn.compile(optimizer='adam', \n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.datasets' has no attribute 'mnist_corrupted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/danielzhang/Projects/alzheimers_sinai/src/benchmark.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/danielzhang/Projects/alzheimers_sinai/src/benchmark.ipynb#ch0000002?line=0'>1</a>\u001b[0m mnist \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mmnist_corrupted\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/danielzhang/Projects/alzheimers_sinai/src/benchmark.ipynb#ch0000002?line=2'>3</a>\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39mload_data()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/danielzhang/Projects/alzheimers_sinai/src/benchmark.ipynb#ch0000002?line=3'>4</a>\u001b[0m x_train, x_test \u001b[39m=\u001b[39m x_train \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m, x_test \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.datasets' has no attribute 'mnist_corrupted'"
     ]
    }
   ],
   "source": [
    "mnist = tfds.image_classification.MNISTCorrupted\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(60000, 28**2)\n",
    "x_test = x_test.reshape(10000, 28**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "772e58729639664bc7c39167d4bac503b22bfc07fa21a50b41389124601dcd2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
