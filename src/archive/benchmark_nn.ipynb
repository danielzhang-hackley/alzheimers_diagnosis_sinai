{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/X_expr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../data/X_expr.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mseqLibID\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      2\u001b[0m y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../data/y_cog.csv\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mseqLibID\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m y \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39mfit_transform(y\u001b[39m.\u001b[39mravel())\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m     f,\n\u001b[0;32m   1220\u001b[0m     mode,\n\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/X_expr.csv'"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(r'../data/X_expr.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = pd.read_csv(r'../data/y_cog.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = label_encoder.fit_transform(y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train, y_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, y_train)\n",
    ").shuffle(10000).batch(100)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_encoder(input_shape, layer_sizes, activation='relu'):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    for i, n_nodes in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(input_layer)\n",
    "        else:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_cnn_encoder(input_shape, filters, kernel_sizes, dnn_layer_sizes, strides=None, paddings=\"valid\"):\n",
    "    if strides is None:\n",
    "        strides = [(1, 1) for _ in range(len(filters))]\n",
    "\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    for i in range(len(filters)):\n",
    "        if i == 0:\n",
    "            x = layers.Conv2D(filters[i], kernel_sizes[i], strides[i])(input_layer)\n",
    "        else:\n",
    "            x = layers.Conv2D(filters[i], kernel_sizes[i], strides[i])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    pre_flatten_idx = len(filters) - 1\n",
    "    flatten_idx = len(filters)\n",
    "\n",
    "    for i in range(len(dnn_layer_sizes)):\n",
    "        x = layers.Dense(dnn_layer_sizes[i])(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    pre_flatten_dim = model.layers[pre_flatten_idx].output_shape\n",
    "    n_flatten_nodes = model.layers[flatten_idx].output_shape\n",
    "\n",
    "    return model, pre_flatten_dim, n_flatten_nodes\n",
    "\n",
    "\n",
    "def get_rf_encoder(X_train, y_train, n_estimators=1000, random_state=None):\n",
    "    \"\"\"\n",
    "    trees = model.estimators_\n",
    "    # Get all 50 tree predictions for the first sample in X_train\n",
    "    preds_for_0 = [tree.predict(X_train[0].reshape(1, -1))[0] for tree in trees]\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_decoder(latent_shape, layer_sizes, activation='relu'):\n",
    "    \"\"\"\n",
    "    Not strictly for decoding; can also be used for classification\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=latent_shape)\n",
    "    for i, n_nodes in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(input_layer)\n",
    "        elif i == len(layer_sizes) - 1:\n",
    "            x = layers.Dense(n_nodes, activation=tf.keras.activations.softmax)(x)\n",
    "        else:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_cnn_decoder(latent_shape, pre_flatten_dim, ):\n",
    "    pass\n",
    "\n",
    "def get_svm_decoder(latent_shape):\n",
    "    pass\n",
    "\n",
    "def get_logistic_decoder(latent_shape):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "loss_object_sparse = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "loss_object_autoencoder = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(data, training=True)\n",
    "        loss = loss_object_sparse(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy_sparse(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, data, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = loss_object_sparse(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy_sparse(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_autoencoder(encoder, decoder, data, labels):\n",
    "    with tf.GradientTape() as enc, tf.GradientTape() as dec:\n",
    "        latent = encoder(data, training=True)\n",
    "        generated_data = decoder(latent, training=True)\n",
    "        loss = loss_object_autoencoder(data, generated_data)\n",
    "    encoder_gradient = enc.gradient(loss, encoder.trainable_variables)\n",
    "    decoder_gradient = dec.gradient(loss, decoder.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(encoder_gradient, encoder.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(decoder_gradient, decoder.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_AE = 50\n",
    "\n",
    "layer_sizes = [18980, 10000, 2500, 2500, 400]\n",
    "layer_sizes_reversed = list(reversed(layer_sizes))\n",
    "encoder = get_nn_encoder((layer_sizes[0]), layer_sizes=layer_sizes[1:])\n",
    "decoder = get_nn_decoder((layer_sizes_reversed[0]), layer_sizes=layer_sizes_reversed[1:])\n",
    "\n",
    "for epoch in range(EPOCHS_AE):\n",
    "    start_time = time.time()\n",
    "    for data, labels in train_ds:\n",
    "        loss = train_step_autoencoder(encoder, decoder, data, labels)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss}, Time for epoch: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "# model = get_nn_decoder((400), layer_sizes=[300, 150, 150, 50, 3])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy_sparse.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy_sparse.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = encoder.predict(data)\n",
    "        train_step(model, encoded_data, labels)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = encoder.predict(test_data)\n",
    "        test_step(model, encoded_test_data, test_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy_sparse.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy_sparse.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in train_ds:\n",
    "    print(type(data))\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 1000\n",
    "rf_encoder = get_rf_encoder(X_train, y_train, n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0200287103652954, Accuracy: 43.1297721862793, Test Loss: 1.1670171022415161, Test Accuracy: 36.66666793823242\n",
      "Epoch 2, Loss: 0.9879631400108337, Accuracy: 43.511451721191406, Test Loss: 1.168146014213562, Test Accuracy: 40.0\n",
      "Epoch 3, Loss: 0.9739248156547546, Accuracy: 43.511451721191406, Test Loss: 1.1680554151535034, Test Accuracy: 40.0\n",
      "Epoch 4, Loss: 0.9544284343719482, Accuracy: 43.89312744140625, Test Loss: 1.1672734022140503, Test Accuracy: 40.0\n",
      "Epoch 5, Loss: 0.9476329684257507, Accuracy: 44.274810791015625, Test Loss: 1.1661981344223022, Test Accuracy: 40.0\n",
      "Epoch 6, Loss: 0.9412970542907715, Accuracy: 44.274810791015625, Test Loss: 1.1646977663040161, Test Accuracy: 40.0\n",
      "Epoch 7, Loss: 0.9238945841789246, Accuracy: 45.03816604614258, Test Loss: 1.1619668006896973, Test Accuracy: 40.0\n",
      "Epoch 8, Loss: 0.9109888076782227, Accuracy: 45.41984939575195, Test Loss: 1.1606254577636719, Test Accuracy: 40.0\n",
      "Epoch 9, Loss: 0.9015046954154968, Accuracy: 45.8015251159668, Test Loss: 1.1602338552474976, Test Accuracy: 40.0\n",
      "Epoch 10, Loss: 0.8901289105415344, Accuracy: 46.183204650878906, Test Loss: 1.159968614578247, Test Accuracy: 40.0\n",
      "Epoch 11, Loss: 0.8926048874855042, Accuracy: 46.183204650878906, Test Loss: 1.1605316400527954, Test Accuracy: 40.0\n",
      "Epoch 12, Loss: 0.8686785697937012, Accuracy: 46.564884185791016, Test Loss: 1.1594433784484863, Test Accuracy: 40.0\n",
      "Epoch 13, Loss: 0.8580775260925293, Accuracy: 47.3282470703125, Test Loss: 1.1582224369049072, Test Accuracy: 40.0\n",
      "Epoch 14, Loss: 0.8574316501617432, Accuracy: 47.709922790527344, Test Loss: 1.1560884714126587, Test Accuracy: 40.0\n",
      "Epoch 15, Loss: 0.8558631539344788, Accuracy: 47.709922790527344, Test Loss: 1.1530802249908447, Test Accuracy: 40.0\n",
      "Epoch 16, Loss: 0.8479328751564026, Accuracy: 48.85496139526367, Test Loss: 1.1500109434127808, Test Accuracy: 40.0\n",
      "Epoch 17, Loss: 0.8507412075996399, Accuracy: 49.61832046508789, Test Loss: 1.1473147869110107, Test Accuracy: 40.0\n",
      "Epoch 18, Loss: 0.8336160778999329, Accuracy: 50.38167953491211, Test Loss: 1.1448190212249756, Test Accuracy: 40.0\n",
      "Epoch 19, Loss: 0.8316817283630371, Accuracy: 50.76335525512695, Test Loss: 1.1440602540969849, Test Accuracy: 40.0\n",
      "Epoch 20, Loss: 0.8293824195861816, Accuracy: 51.52671813964844, Test Loss: 1.1432433128356934, Test Accuracy: 40.0\n",
      "Epoch 21, Loss: 0.8260741233825684, Accuracy: 51.52671813964844, Test Loss: 1.1414917707443237, Test Accuracy: 40.0\n",
      "Epoch 22, Loss: 0.8085289597511292, Accuracy: 51.90839767456055, Test Loss: 1.1401361227035522, Test Accuracy: 43.33333206176758\n",
      "Epoch 23, Loss: 0.8087838292121887, Accuracy: 52.290077209472656, Test Loss: 1.1405762434005737, Test Accuracy: 43.33333206176758\n",
      "Epoch 24, Loss: 0.8014623522758484, Accuracy: 52.6717529296875, Test Loss: 1.1411129236221313, Test Accuracy: 43.33333206176758\n",
      "Epoch 25, Loss: 0.7958753108978271, Accuracy: 53.816795349121094, Test Loss: 1.141643762588501, Test Accuracy: 43.33333206176758\n"
     ]
    }
   ],
   "source": [
    "# 620, -620\n",
    "EPOCHS = 25\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "decoder = get_nn_decoder((1000),  [500, 200, 100, 10, 3])\n",
    "\n",
    "\n",
    "def rf_encode(encoder, data):\n",
    "    \"\"\"\n",
    "    trees = model.estimators_\n",
    "    # Get all 50 tree predictions for the first sample in X_train\n",
    "    preds_for_0 = [tree.predict(X_train[0].reshape(1, -1))[0] for tree in trees]\n",
    "    \"\"\"\n",
    "    trees = encoder.estimators_\n",
    "    n_estimators = len(trees)\n",
    "\n",
    "    output = np.zeros(shape=(data.shape[0], n_estimators))\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        output[i] = [tree.predict(sample.numpy().reshape(1, -1))[0] for tree in trees]\n",
    "\n",
    "    return tf.convert_to_tensor(output)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy_sparse.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy_sparse.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = rf_encode(rf_encoder, data)\n",
    "        train_step(decoder, encoded_data, labels)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = rf_encode(rf_encoder, test_data)\n",
    "        test_step(decoder, encoded_test_data, test_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy_sparse.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy_sparse.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "772e58729639664bc7c39167d4bac503b22bfc07fa21a50b41389124601dcd2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
