{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'../data/X_expr.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = pd.read_csv(r'../data/y_cog.csv').drop(['Unnamed: 0', 'seqLibID'], axis=1).values\n",
    "y = label_encoder.fit_transform(y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train = min_max_scaler.fit_transform(X_train, y_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, y_train)\n",
    ").shuffle(10000).batch(100)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rf_encoder(X_train, y_train, n_estimators=1000, random_state=None):\n",
    "    \"\"\"\n",
    "    trees = model.estimators_\n",
    "    # Get all 50 tree predictions for the first sample in X_train\n",
    "    preds_for_0 = [tree.predict(X_train[0].reshape(1, -1))[0] for tree in trees]\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    else:\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_nn_decoder(latent_shape, layer_sizes, dropout_rates=None, activation='relu'):\n",
    "    \"\"\"\n",
    "    Not strictly for decoding; can also be used for classification\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=latent_shape)\n",
    "    for i, n_nodes in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(input_layer)\n",
    "            if dropout_rates is not None:\n",
    "                x = layers.Dropout(dropout_rates[i])(x)\n",
    "        elif i == len(layer_sizes) - 1:\n",
    "            x = layers.Dense(n_nodes, activation=tf.keras.activations.softmax)(x)\n",
    "        else:\n",
    "            x = layers.Dense(n_nodes, activation=activation)(x)\n",
    "            if dropout_rates is not None:\n",
    "                x = layers.Dropout(dropout_rates[i])(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object_sparse = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "loss_object_autoencoder = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy_sparse = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(data, training=True)\n",
    "        loss = loss_object_sparse(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy_sparse(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, data, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = loss_object_sparse(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy_sparse(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 1000\n",
    "rf_encoder = get_rf_encoder(X_train, y_train, n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92.   0.   1.]\n",
      " [ 63.   0.   3.]\n",
      " [100.   0.   3.]]\n",
      "[[11.  0.  0.]\n",
      " [ 7.  0.  0.]\n",
      " [11.  0.  1.]]\n",
      "Epoch 1, Loss: 1.6438946723937988, Accuracy: 35.496185302734375, Test Loss: 1.461470365524292, Test Accuracy: 40.0\n",
      "[[92.  0.  1.]\n",
      " [57.  0.  9.]\n",
      " [82.  0. 21.]]\n",
      "[[10.  0.  1.]\n",
      " [ 7.  0.  0.]\n",
      " [11.  0.  1.]]\n",
      "Epoch 2, Loss: 1.4226804971694946, Accuracy: 38.931297302246094, Test Loss: 1.3736521005630493, Test Accuracy: 36.66666793823242\n",
      "[[91.  0.  2.]\n",
      " [50.  0. 16.]\n",
      " [62.  0. 41.]]\n",
      "[[10.  0.  1.]\n",
      " [ 7.  0.  0.]\n",
      " [11.  0.  1.]]\n",
      "Epoch 3, Loss: 1.2763837575912476, Accuracy: 45.8015251159668, Test Loss: 1.2977794408798218, Test Accuracy: 36.66666793823242\n",
      "[[88.  2.  3.]\n",
      " [43.  0. 23.]\n",
      " [48.  0. 55.]]\n",
      "[[ 9.  0.  2.]\n",
      " [ 7.  0.  0.]\n",
      " [11.  0.  1.]]\n",
      "Epoch 4, Loss: 1.166983962059021, Accuracy: 51.90839767456055, Test Loss: 1.2324671745300293, Test Accuracy: 33.333335876464844\n",
      "[[88.  2.  3.]\n",
      " [36.  0. 30.]\n",
      " [34.  1. 68.]]\n",
      "[[9. 0. 2.]\n",
      " [7. 0. 0.]\n",
      " [7. 0. 5.]]\n",
      "Epoch 5, Loss: 1.0959889888763428, Accuracy: 55.72519302368164, Test Loss: 1.1814738512039185, Test Accuracy: 46.66666793823242\n",
      "[[88.  2.  3.]\n",
      " [25.  0. 41.]\n",
      " [24.  1. 78.]]\n",
      "[[8. 1. 2.]\n",
      " [5. 0. 2.]\n",
      " [6. 0. 6.]]\n",
      "Epoch 6, Loss: 1.0447949171066284, Accuracy: 59.5419807434082, Test Loss: 1.1435177326202393, Test Accuracy: 46.66666793823242\n",
      "[[87.  2.  4.]\n",
      " [17.  1. 48.]\n",
      " [18.  1. 84.]]\n",
      "[[6. 1. 4.]\n",
      " [5. 0. 2.]\n",
      " [5. 0. 7.]]\n",
      "Epoch 7, Loss: 0.9909946918487549, Accuracy: 63.3587760925293, Test Loss: 1.1128476858139038, Test Accuracy: 43.33333206176758\n",
      "[[86.  3.  4.]\n",
      " [10.  2. 54.]\n",
      " [ 8.  2. 93.]]\n",
      "[[5. 1. 5.]\n",
      " [4. 0. 3.]\n",
      " [4. 0. 8.]]\n",
      "Epoch 8, Loss: 0.9657747149467468, Accuracy: 67.93892669677734, Test Loss: 1.0904024839401245, Test Accuracy: 43.33333206176758\n",
      "[[85.  4.  4.]\n",
      " [ 9.  3. 54.]\n",
      " [ 6.  4. 93.]]\n",
      "[[5. 1. 5.]\n",
      " [4. 0. 3.]\n",
      " [4. 0. 8.]]\n",
      "Epoch 9, Loss: 0.9269316792488098, Accuracy: 68.32061004638672, Test Loss: 1.0753024816513062, Test Accuracy: 43.33333206176758\n",
      "[[85.  4.  4.]\n",
      " [ 8.  4. 54.]\n",
      " [ 3.  4. 96.]]\n",
      "[[5. 2. 4.]\n",
      " [4. 0. 3.]\n",
      " [4. 0. 8.]]\n",
      "Epoch 10, Loss: 0.9129671454429626, Accuracy: 69.84732818603516, Test Loss: 1.0675936937332153, Test Accuracy: 43.33333206176758\n",
      "[[85.  4.  4.]\n",
      " [ 3.  3. 60.]\n",
      " [ 1.  3. 99.]]\n",
      "[[5. 2. 4.]\n",
      " [4. 0. 3.]\n",
      " [4. 0. 8.]]\n",
      "Epoch 11, Loss: 0.9032896161079407, Accuracy: 71.3740463256836, Test Loss: 1.062969446182251, Test Accuracy: 43.33333206176758\n",
      "[[ 85.   4.   4.]\n",
      " [  3.   3.  60.]\n",
      " [  0.   3. 100.]]\n",
      "[[5. 2. 4.]\n",
      " [3. 0. 4.]\n",
      " [4. 1. 7.]]\n",
      "Epoch 12, Loss: 0.8825635313987732, Accuracy: 71.3740463256836, Test Loss: 1.0603805780410767, Test Accuracy: 40.0\n",
      "[[ 84.   4.   5.]\n",
      " [  2.   2.  62.]\n",
      " [  0.   3. 100.]]\n",
      "[[5. 2. 4.]\n",
      " [2. 0. 5.]\n",
      " [4. 1. 7.]]\n",
      "Epoch 13, Loss: 0.8716928362846375, Accuracy: 71.75572204589844, Test Loss: 1.059885859489441, Test Accuracy: 40.0\n",
      "[[ 85.   4.   4.]\n",
      " [  2.   1.  63.]\n",
      " [  0.   2. 101.]]\n",
      "[[5. 2. 4.]\n",
      " [2. 0. 5.]\n",
      " [4. 1. 7.]]\n",
      "Epoch 14, Loss: 0.8440009951591492, Accuracy: 70.99237060546875, Test Loss: 1.0603114366531372, Test Accuracy: 40.0\n",
      "[[ 86.   3.   4.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   1. 102.]]\n",
      "[[5. 2. 4.]\n",
      " [2. 0. 5.]\n",
      " [2. 1. 9.]]\n",
      "Epoch 15, Loss: 0.8427426218986511, Accuracy: 71.3740463256836, Test Loss: 1.061881422996521, Test Accuracy: 46.66666793823242\n",
      "[[ 87.   3.   3.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   1. 102.]]\n",
      "[[5. 2. 4.]\n",
      " [2. 0. 5.]\n",
      " [2. 1. 9.]]\n",
      "Epoch 16, Loss: 0.8260228633880615, Accuracy: 72.13740539550781, Test Loss: 1.0638506412506104, Test Accuracy: 46.66666793823242\n",
      "[[ 88.   2.   3.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   0. 103.]]\n",
      "[[5. 2. 4.]\n",
      " [2. 0. 5.]\n",
      " [2. 1. 9.]]\n",
      "Epoch 17, Loss: 0.8142273426055908, Accuracy: 72.51908111572266, Test Loss: 1.0666263103485107, Test Accuracy: 46.66666793823242\n",
      "[[ 89.   1.   3.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 18, Loss: 0.8071935176849365, Accuracy: 73.2824478149414, Test Loss: 1.0699317455291748, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 19, Loss: 0.7907085418701172, Accuracy: 73.66412353515625, Test Loss: 1.072542667388916, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 20, Loss: 0.7905710339546204, Accuracy: 73.66412353515625, Test Loss: 1.0748742818832397, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 21, Loss: 0.7847853302955627, Accuracy: 73.66412353515625, Test Loss: 1.0771783590316772, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 22, Loss: 0.7653796076774597, Accuracy: 73.66412353515625, Test Loss: 1.0788524150848389, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 23, Loss: 0.7603721618652344, Accuracy: 73.66412353515625, Test Loss: 1.081291675567627, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 24, Loss: 0.7422678470611572, Accuracy: 73.66412353515625, Test Loss: 1.083353877067566, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[ 5.  1.  5.]\n",
      " [ 1.  0.  6.]\n",
      " [ 2.  0. 10.]]\n",
      "Epoch 25, Loss: 0.7422824501991272, Accuracy: 73.66412353515625, Test Loss: 1.0859181880950928, Test Accuracy: 50.0\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[5. 1. 5.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 26, Loss: 0.7314262390136719, Accuracy: 73.66412353515625, Test Loss: 1.0882350206375122, Test Accuracy: 46.66666793823242\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[5. 1. 5.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 27, Loss: 0.7213303446769714, Accuracy: 73.66412353515625, Test Loss: 1.0905348062515259, Test Accuracy: 46.66666793823242\n",
      "[[ 90.   1.   2.]\n",
      " [  0.   0.  66.]\n",
      " [  0.   0. 103.]]\n",
      "[[5. 1. 5.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 28, Loss: 0.7056174278259277, Accuracy: 73.66412353515625, Test Loss: 1.0927016735076904, Test Accuracy: 46.66666793823242\n",
      "[[ 91.   1.   1.]\n",
      " [  0.   1.  65.]\n",
      " [  0.   0. 103.]]\n",
      "[[5. 1. 5.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 29, Loss: 0.7049379348754883, Accuracy: 74.0457992553711, Test Loss: 1.094357967376709, Test Accuracy: 46.66666793823242\n",
      "[[ 91.   1.   1.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   0. 103.]]\n",
      "[[4. 1. 6.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 30, Loss: 0.7000510096549988, Accuracy: 74.42748260498047, Test Loss: 1.095342993736267, Test Accuracy: 43.33333206176758\n",
      "[[ 91.   1.   1.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   0. 103.]]\n",
      "[[4. 1. 6.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 31, Loss: 0.7013754844665527, Accuracy: 74.42748260498047, Test Loss: 1.0962330102920532, Test Accuracy: 43.33333206176758\n",
      "[[ 91.   1.   1.]\n",
      " [  1.   1.  64.]\n",
      " [  0.   0. 103.]]\n",
      "[[4. 1. 6.]\n",
      " [1. 0. 6.]\n",
      " [3. 0. 9.]]\n",
      "Epoch 32, Loss: 0.6688503623008728, Accuracy: 74.42748260498047, Test Loss: 1.0973665714263916, Test Accuracy: 43.33333206176758\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m test_accuracy_sparse\u001b[39m.\u001b[39mreset_states()\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m data, labels \u001b[39min\u001b[39;00m train_ds:\n\u001b[1;32m---> 32\u001b[0m     encoded_data \u001b[39m=\u001b[39m rf_encode(rf_encoder, data)\n\u001b[0;32m     33\u001b[0m     train_step(decoder, encoded_data, labels)\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m test_data, test_labels \u001b[39min\u001b[39;00m test_ds:\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mrf_encode\u001b[1;34m(encoder, data)\u001b[0m\n\u001b[0;32m     16\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(shape\u001b[39m=\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], n_estimators))\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n\u001b[1;32m---> 19\u001b[0m     output[i] \u001b[39m=\u001b[39m [tree\u001b[39m.\u001b[39mpredict(sample\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m trees]\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(shape\u001b[39m=\u001b[39m(data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], n_estimators))\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n\u001b[1;32m---> 19\u001b[0m     output[i] \u001b[39m=\u001b[39m [tree\u001b[39m.\u001b[39mpredict(sample\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m trees]\n\u001b[0;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mconvert_to_tensor(output)\n",
      "File \u001b[1;32mc:\\Users\\danie\\Projects\\alzheimers_sinai\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39;49mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 620, -620\n",
    "EPOCHS = 250\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "decoder = get_nn_decoder((1000),  [500, 200, 100, 10, 3])\n",
    "\n",
    "def rf_encode(encoder, data):\n",
    "    \"\"\"\n",
    "    trees = model.estimators_\n",
    "    # Get all 50 tree predictions for the first sample in X_train\n",
    "    preds_for_0 = [tree.predict(X_train[0].reshape(1, -1))[0] for tree in trees]\n",
    "    \"\"\"\n",
    "    trees = encoder.estimators_\n",
    "    n_estimators = len(trees)\n",
    "\n",
    "    output = np.zeros(shape=(data.shape[0], n_estimators))\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        output[i] = [tree.predict(sample.numpy().reshape(1, -1))[0] for tree in trees]\n",
    "\n",
    "    return tf.convert_to_tensor(output)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy_sparse.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy_sparse.reset_states()\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = rf_encode(rf_encoder, data)\n",
    "        train_step(decoder, encoded_data, labels)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = rf_encode(rf_encoder, test_data)\n",
    "        test_step(decoder, encoded_test_data, test_labels)\n",
    "\n",
    "    train_cnf = np.zeros(shape=(3, 3))\n",
    "    test_cnf = np.zeros(shape=(3, 3))\n",
    "\n",
    "    for data, labels in train_ds:\n",
    "        encoded_data = rf_encode(rf_encoder, data)\n",
    "        predictions = decoder(encoded_data).numpy().argmax(axis=1)\n",
    "        train_cnf += confusion_matrix(labels, predictions)\n",
    "\n",
    "    for test_data, test_labels in test_ds:\n",
    "        encoded_test_data = rf_encode(rf_encoder, test_data)\n",
    "        predictions_test = decoder(encoded_test_data).numpy().argmax(axis=1)\n",
    "        test_cnf += confusion_matrix(test_labels, predictions_test)\n",
    "\n",
    "    print(train_cnf)\n",
    "    print(test_cnf)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy_sparse.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy_sparse.result() * 100}'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "772e58729639664bc7c39167d4bac503b22bfc07fa21a50b41389124601dcd2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
