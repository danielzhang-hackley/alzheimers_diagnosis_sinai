{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "def fit_transform_one_hot(arr):\n",
    "    output = np.zeros([arr.shape[0]])\n",
    "    for i in range(len(output)):\n",
    "        output[i] = np.argmax(arr[i])\n",
    "    return output\n",
    "\n",
    "label_encoder.fit_transform_one_hot = fit_transform_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 7s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "73\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "print(np.where(y == 'NoCognitiveImpairment')[0].shape[0])\n",
    "print(np.where(y == 'MildCognitiveImpairment')[0].shape[0])\n",
    "print(np.where(y == 'AD')[0].shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = X_train.shape[1]\n",
    "n_classes = 10  # y_train.shape[1]\n",
    "\n",
    "# 7, 67, 72\n",
    "\n",
    "lr_l1 = LogisticRegression(penalty=\"l2\", multi_class=\"ovr\", solver=\"saga\", max_iter=500)\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "dnn = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(n_classes)\n",
    "])\n",
    "dnn.compile(optimizer='adam', \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "classical_models = (lr_l1, svm, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3333333333333333\n",
      "[[4 6 6]\n",
      " [0 3 5]\n",
      " [2 1 3]]\n",
      "********************\n",
      "acc: 0.2\n",
      "[[ 0  0 16]\n",
      " [ 0  0  8]\n",
      " [ 0  0  6]]\n",
      "********************\n",
      "acc: 0.3333333333333333\n",
      "[[ 4  0 12]\n",
      " [ 1  0  7]\n",
      " [ 0  0  6]]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "y_train_sparse = label_encoder.fit_transform(y_train.ravel())\n",
    "y_test_sparse = label_encoder.transform(y_test.ravel())\n",
    "\n",
    "\n",
    "for model in classical_models:\n",
    "    model.fit(X_train_normalized, y_train_sparse)\n",
    "    y_pred = model.predict(X_test_normalized).ravel()\n",
    "    \n",
    "    cnf = confusion_matrix(y_test_sparse, y_pred)\n",
    "    acc = accuracy_score(y_test_sparse, y_pred)\n",
    "    # f1 = f1_score(y_test_sparse, y_pred)\n",
    "\n",
    "    print(f\"acc: {acc}\")  # , f1: {f1}\")\n",
    "    print(cnf)\n",
    "    print(\"********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 687us/step - loss: 3.5651 - accuracy: 0.6921\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 690us/step - loss: 0.7272 - accuracy: 0.7341\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 692us/step - loss: 0.6249 - accuracy: 0.7684\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 696us/step - loss: 0.5534 - accuracy: 0.8051\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 689us/step - loss: 0.5205 - accuracy: 0.8178\n",
      "1875/1875 [==============================] - 1s 449us/step\n",
      "313/313 [==============================] - 0s 459us/step\n",
      "acc: 0.8253833333333334\n",
      "[[4962   26   25  157    3    1 1043    0   10    0]\n",
      " [   2 5687    9   19    4    0    3    0    0    0]\n",
      " [ 351   78 5130  192 1050   14 1447    3   80   18]\n",
      " [ 403  195   88 5456  553    4  268    0   38    2]\n",
      " [   3    0  289   74 3479    0  411    0    4    0]\n",
      " [   0    0    1    0    3 5530    2  153    5  185]\n",
      " [ 212   12  427   88  834    4 2370    2   62    4]\n",
      " [   0    0    0    0    0  199    0 5716   14  373]\n",
      " [  67    2   31   14   74   78  456   19 5787   12]\n",
      " [   0    0    0    0    0  170    0  107    0 5406]]\n",
      "********************\n",
      "acc: 0.8137\n",
      "[[798   4   8  30   0   0 166   0   0   0]\n",
      " [  1 939   0   5   1   0   1   0   0   0]\n",
      " [ 71  14 832  43 183   3 249   1  17   3]\n",
      " [ 73  36  15 886  84   0  53   0   8   0]\n",
      " [  1   1  54  11 590   0  69   0   2   0]\n",
      " [  0   0   0   0   1 908   0  36   3  27]\n",
      " [ 47   4  86  20 125   2 383   0  16   0]\n",
      " [  0   0   0   0   0  40   0 952   5  68]\n",
      " [  9   2   5   5  16   9  79   0 949   2]\n",
      " [  0   0   0   0   0  38   0  11   0 900]]\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "dnn.fit(X_train, y_train, epochs=5)\n",
    "y_pred_train = dnn.predict(X_train)\n",
    "y_pred_test = dnn.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred_train_sparse = label_encoder.fit_transform_one_hot(y_pred_train)\n",
    "y_pred_test_sparse = label_encoder.fit_transform_one_hot(y_pred_test)\n",
    "\n",
    "\n",
    "cnf_train = confusion_matrix(y_pred_train_sparse, y_train)\n",
    "acc_train = accuracy_score(y_pred_train_sparse, y_train)\n",
    "\n",
    "print(f\"acc: {acc_train}\")\n",
    "print(cnf_train)\n",
    "print(\"********************\")\n",
    "\n",
    "cnf_test = confusion_matrix(y_pred_test_sparse, y_test)\n",
    "acc_test = accuracy_score(y_pred_test_sparse, y_test)\n",
    "\n",
    "print(f\"acc: {acc_test}\")\n",
    "print(cnf_test)\n",
    "print(\"********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00958006,  0.00844916, -0.00099614, ..., -0.01144092,\n",
       "         0.00686088, -0.00235485],\n",
       "       [ 0.00958006,  0.00844916, -0.00099614, ..., -0.01144092,\n",
       "         0.00686088, -0.00235485],\n",
       "       [ 0.00958006,  0.00844916, -0.00099614, ..., -0.01144092,\n",
       "         0.00686088, -0.00235485],\n",
       "       ...,\n",
       "       [ 0.00958006,  0.00844916, -0.00099614, ..., -0.01144092,\n",
       "         0.00686088, -0.00235485],\n",
       "       [ 0.00958006,  0.00844916, -0.00099614, ..., -0.01144092,\n",
       "         0.00686088, -0.00235485],\n",
       "       [ 0.00958006,  0.00844916, -0.00099614, ..., -0.01144092,\n",
       "         0.00686088, -0.00235485]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(\u001b[43my_train_sparse\u001b[49m)\n\u001b[1;32m      2\u001b[0m y_test_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(y_test_sparse)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(y_train_one_hot)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_sparse' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_one_hot = to_categorical(y_train_sparse)\n",
    "y_test_one_hot = to_categorical(y_test_sparse)\n",
    "# print(y_train_one_hot)\n",
    "\n",
    "dnn.fit(X_train, y_train_one_hot, epochs=5)\n",
    "y_pred_train = dnn.predict(X_train)\n",
    "y_pred = dnn.predict(X_test)\n",
    "\n",
    "y_pred_sparse_train = label_encoder.fit_transform_one_hot(y_pred_train)\n",
    "y_pred_sparse = label_encoder.fit_transform_one_hot(y_pred)\n",
    "\n",
    "cnf_train = confusion_matrix(y_train_sparse, y_pred_sparse_train)\n",
    "acc_train = accuracy_score(y_train_sparse, y_pred_sparse_train)\n",
    "\n",
    "print(f\"acc: {acc_train}\")\n",
    "print(cnf_train)\n",
    "print(\"********************\")\n",
    "\n",
    "cnf_test = confusion_matrix(y_test_sparse, y_pred_sparse)\n",
    "acc_test = accuracy_score(y_test_sparse, y_pred_sparse)\n",
    "\n",
    "print(f\"acc: {acc_test}\")\n",
    "print(cnf_test)\n",
    "print(\"********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_test_sparse == 2)[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielzhang/Projects/alzheimers_diagnosis_sinai/venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def encoder(input_encoder):\n",
    "\tinputs = keras.Input(shape=input_encoder, name='input_layer')\n",
    "\n",
    "\t# Block-1\n",
    "\tx = layers.Dense(100, name='conv_1')(inputs)\n",
    "\tx = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "\n",
    "\t# Block-2\n",
    "\tx = layers.Dense(75, name='conv_2')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "\n",
    "\t# Block-3\n",
    "\tx = layers.Dense(75, name='conv_3')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "\n",
    "\n",
    "\t# Block-4\n",
    "\tx = layers.Dense(50, name='conv_4')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_4')(x)\n",
    "\n",
    "\t# Final Block\n",
    "\tflatten = x\n",
    "\tmean = layers.Dense(2, name='mean')(flatten)\n",
    "\tlog_var = layers.Dense(2, name='log_var')(flatten)\n",
    "\tmodel = tf.keras.Model(inputs, (mean, log_var), name=\"Encoder\")\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def sampling(input_1,input_2):\n",
    "\tmean = keras.Input(shape=input_1, name='input_layer1')\n",
    "\tlog_var = keras.Input(shape=input_2, name='input_layer2')\n",
    "\tout = layers.Lambda(sampling_reparameterization_model, name='encoder_output')([mean, log_var])\n",
    "\tenc_2 = tf.keras.Model([mean,log_var], out,  name=\"Encoder_2\")\n",
    "\n",
    "\treturn enc_2\n",
    "\n",
    "\n",
    "def sampling_reparameterization_model(distribution_params):\n",
    "    mean, log_var = distribution_params\n",
    "    epsilon = K.random_normal(shape=K.shape(mean), mean=0., stddev=1.)\n",
    "    z = mean + K.exp(log_var / 2) * epsilon\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "# replace the decoder with the supervised learning method\n",
    "def decoder(input_decoder):\n",
    "\t\t\n",
    "\tinputs = keras.Input(shape=input_decoder, name='input_layer')\n",
    "\tx = layers.Dense(50, name='dense_1')(inputs)\n",
    "\t\n",
    "\t# Block-1\n",
    "\tx = layers.Dense(75,name='conv_transpose_1')(x)\n",
    "\tx = layers.BatchNormalization(name='bn_1')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "\t\n",
    "\t# Block-2\n",
    "\tx = layers.Dense(75, name='conv_transpose_2')(x)\n",
    "\tx = layers.BatchNormalization(name='bn_2')(x)\n",
    "\tx = layers.LeakyReLU(name='lrelu_2')(x)\t\t\n",
    "\n",
    "\t# INSERT NORMAL SUPERVISED LEARNING HERE?\n",
    "\t# Block-4\n",
    "\toutputs = layers.Dense(100, name='conv_transpose_4')(x)\n",
    "\tmodel = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
    "\treturn model\t\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 0.0005)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "\tr_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\treturn 1000 * r_loss\n",
    "\t\n",
    "\n",
    "def kl_loss(mean, log_var):\n",
    "\tkl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis = 1)\n",
    "\treturn kl_loss\n",
    "\t\n",
    "\n",
    "def vae_loss(y_true, y_pred, mean, log_var):\n",
    "\tr_loss = mse_loss(y_true, y_pred)\n",
    "\tkl_loss = kl_loss(mean, log_var)\n",
    "\treturn  r_loss + kl_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "\t\n",
    "\twith tf.GradientTape() as enc, tf.GradientTape() as dec:\n",
    "\t\t\n",
    "\t\tmean, log_var = enc(images, training=True)\n",
    "\t\tlatent = sampling([mean, log_var])\n",
    "\t\tgenerated_images = dec(latent, training=True)\n",
    "\t\tloss = vae_loss(images, generated_images, mean, log_var)\n",
    "\t\n",
    "\t\t\t\n",
    "\tgradients_of_enc = encoder.gradient(loss, enc.trainable_variables)\n",
    "\tgradients_of_dec = decoder.gradient(loss, dec.trainable_variables)\n",
    "\t\t\n",
    "\t\t\n",
    "\toptimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
    "\toptimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
    "\treturn loss\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tstart = time.time()\n",
    "\t\tfor image_batch in dataset:\n",
    "\t\t\ttrain_step(image_batch)\n",
    "\t\n",
    "\tprint ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb92a5de44aaf59754d1a7f59abdbfeec2642db842c8064b433264bae7e1adfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
