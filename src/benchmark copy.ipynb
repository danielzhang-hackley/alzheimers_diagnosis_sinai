{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 7s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDNN(tf.keras.Model):\n",
    "    def __init__(self, node_counts, dropout_ratios=None, \n",
    "                 activation='relu', optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)):\n",
    "        super(BaseDNN, self).init__()\n",
    "        self.node_counts = node_counts\n",
    "        self.dropout_ratios = [0 for _ in range(len(self.node_counts))] if dropout_ratios is None else dropout_ratios\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.optimizer=optimizer\n",
    "        self.loss=loss\n",
    "\n",
    "    def call(self, x):\n",
    "        for n_nodes, dropout_ratio in zip(self.node_counts, self.dropout_ratios):\n",
    "            x = layers.Dense(n_nodes, activation=self.activation)(x)\n",
    "            x = layers.Dropout(dropout_ratio)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvolutionalLayers(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_sizes, activation, strides=None, paddings=None,\n",
    "                 optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)):\n",
    "        super(ConvolutionalLayers, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernels_sizes = kernel_sizes\n",
    "        self.activation = activation\n",
    "\n",
    "        if strides is None:\n",
    "            self.strides = []\n",
    "            for _ in range(len(filters)):\n",
    "                self.strides = (1, 1)\n",
    "        else:\n",
    "            self.strides = strides\n",
    "\n",
    "        if paddings is None:\n",
    "            for _ in range(len(filters)):\n",
    "                self.filters = \"valid\"\n",
    "        else:\n",
    "            self.paddings = paddings\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def call(self, x):\n",
    "        for n_filters, kernel_size, stride, padding in zip(self.filters, self.kernels_sizes, self.strides, self.paddings):\n",
    "            x = layers.Conv2D(n_filters, kernel_size, strides=stride, padding=padding, activation=self.activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(data, training=True)\n",
    "        loss = model.loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, data, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = model.loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "model_input = tf.keras.layers.Input(shape=(64,64,3,), name=\"input_layer\")\n",
    "x = ConvolutionalLayers()(model_input)\n",
    "x = BaseDNN()(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=model_input, outputs=x)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in zip(X_train, y_train):\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in zip(X_test, y_test):\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDNN(tf.keras.Model):\n",
    "    def __init__(self, node_counts, dropout_ratios=None, \n",
    "                 activation='relu', optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)):\n",
    "        super(BaseDNN, self).init__()\n",
    "        self.node_counts = node_counts\n",
    "        self.dropout_ratios = [0 for _ in range(len(self.node_counts))] if dropout_ratios is None else dropout_ratios\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.optimizer=optimizer\n",
    "        self.loss=loss\n",
    "\n",
    "    def call(self, x):\n",
    "        for n_nodes, dropout_ratio in zip(self.node_counts, self.dropout_ratios):\n",
    "            x = layers.Dense(n_nodes, activation=self.activation)(x)\n",
    "            x = layers.Dropout(dropout_ratio)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(data, training=True)\n",
    "        loss = model.loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, data, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = model.loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb92a5de44aaf59754d1a7f59abdbfeec2642db842c8064b433264bae7e1adfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
